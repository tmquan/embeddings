# Nemotron Datasets Statistics

Generated from `/raid/datasets` on 2026-01-24.

## Summary

| Dataset | Version | Format | Total Rows | Splits | Files | Strategy |
|---------|---------|--------|------------|--------|-------|----------|
| **Pretraining** |
| Nemotron-Pretraining-Dataset-sample | pretrain | parquet | 27,706 | 10 | 10 | direct_text |
| **Llama-Nemotron** |
| Llama-Nemotron-Post-Training-Dataset | llama-sft | arrow | 32,955,418 | 5 | 247 | input_output |
| **Post-Training v1** |
| Nemotron-Post-Training-Dataset-v1 | v1 | arrow | 25,659,642 | 5 | 1,023 | concatenate_messages |
| **Post-Training v2** |
| Nemotron-Post-Training-Dataset-v2 | v2 | arrow | 6,341,414 | 9 | 201 | concatenate_messages |
| **Post-Training v3** |
| Nemotron-Instruction-Following-Chat-v1 | v3-chat | arrow | 430,978 | 2 | 15 | concatenate_messages |
| Nemotron-Agentic-v1 | v3-agentic | jsonl | 335,122 | 2 | 2 | concatenate_messages |
| Nemotron-Science-v1 | v3-science | arrow | 226,334 | 2 | 6 | concatenate_messages |
| Nemotron-Math-Proofs-v1 | v3-math-proofs | arrow | 1,376,663 | 1 | 58 | combine_columns |
| Nemotron-Math-v2 | v3-math | jsonl | 7,085,839 | 5 | 5 | concatenate_messages |
| Nemotron-3-Nano-RL-Training-Blend | v3-rl-blend | jsonl | 93,244 | 1 | 1 | combine_columns |
| Nemotron-Competitive-Programming-v1 | v3-competitive | jsonl | 3,927,984 | 6 | 6 | concatenate_messages |
| Nemotron-SWE-v1 | v3-swe | arrow | 51,029 | 1 | 21 | concatenate_messages |

**Total: 78,511,373 rows across 12 datasets (1,595 files)**

---

## Detailed Statistics

### nvidia/Nemotron-Pretraining-Dataset-sample (pretrain)

- **Format**: Parquet
- **Total Rows**: 27,706
- **Strategy**: `direct_text`

**Columns**:
- `id`
- `text` (text)

**Splits**:
| Split | Rows | Files | Shards |
|-------|------|-------|--------|
| Nemotron-CC-Diverse-QA | ~2,771 | 1 parquet | 0 |
| Nemotron-CC-High-Quality | ~2,771 | 1 parquet | 0 |
| Nemotron-CC-High-Quality-Synthetic | ~2,771 | 1 parquet | 0 |
| Nemotron-CC-MATH | ~2,771 | 1 parquet | 0 |
| Nemotron-CC-Translated-Diverse-QA | ~2,771 | 1 parquet | 0 |
| Nemotron-Code-Metadata | ~2,771 | 1 parquet | 0 |
| Nemotron-SFT-Code | ~2,771 | 1 parquet | 0 |
| Nemotron-SFT-General | ~2,771 | 1 parquet | 0 |
| Nemotron-SFT-MATH | ~2,771 | 1 parquet | 0 |
| Nemotron-Synthetic-Code | ~2,771 | 1 parquet | 0 |

---

### nvidia/Llama-Nemotron-Post-Training-Dataset (llama-sft)

- **Format**: Arrow (HuggingFace)
- **Total Rows**: 32,955,418
- **Strategy**: `input_output`

**Columns**:
- `input` (text)
- `output` (text)
- `category`
- `license`
- `reasoning`
- `generator`
- `used_in_training`
- `version`
- `system_prompt`

**Splits**:
| Split | Rows | Files | Shards |
|-------|------|-------|--------|
| chat | 39,792 | 1 arrow | 0 |
| code | 10,108,883 | 92 arrow | 0-91 |
| math | 22,066,397 | 141 arrow | 0-140 |
| safety | 31,426 | 1 arrow | 0 |
| science | 708,920 | 12 arrow | 0-11 |

---

### nvidia/Nemotron-Post-Training-Dataset-v1 (v1)

- **Format**: Arrow (HuggingFace)
- **Total Rows**: 25,659,642
- **Total Size**: ~477 GB
- **Strategy**: `concatenate_messages`

**Columns**:
- `uuid`
- `license`
- `generator`
- `version`
- `category`
- `reasoning`
- `messages` (text)
- `metadata`

**Splits**:
| Split | Rows | Files | Shards | Size | Generator |
|-------|------|-------|--------|------|-----------|
| chat | 746,622 | 8 arrow | 0-7 | 3.6G | Qwen3-235B-A22B |
| code | 1,896,395 | 183 arrow | 0-182 | 86G | DeepSeek-R1-0528 |
| math | 2,044,407 | 159 arrow | 0-158 | 74G | DeepSeek-R1-0528 |
| stem | 20,662,167 | 660 arrow | 0-659 | 307G | DeepSeek-R1-0528 |
| tool_calling | 310,051 | 13 arrow | 0-12 | 6.0G | Qwen3-235B-A22B |

**Split Descriptions**:
- **chat**: General conversational data generated by Qwen3
- **code**: Programming and coding tasks with reasoning, generated by DeepSeek-R1
- **math**: Mathematical problem-solving with step-by-step reasoning
- **stem**: Science, technology, engineering, math content (largest split - 80% of dataset)
- **tool_calling**: Function/tool calling examples for agentic capabilities

---

### nvidia/Nemotron-Post-Training-Dataset-v2 (v2)

- **Format**: Arrow (HuggingFace)
- **Total Rows**: 6,341,414
- **Total Size**: ~94 GB
- **Strategy**: `concatenate_messages`

**Columns**:
- `uuid`
- `license`
- `generator`
- `version`
- `category`
- `reasoning`
- `messages` (text)

**Splits**:
| Split | Rows | Files | Shards | Size | Generator |
|-------|------|-------|--------|------|-----------|
| chat | 627,720 | 12 arrow | 0-11 | 5.6G | Qwen3-235B-A22B, Qwen3-30B-A3B |
| code | 175,000 | 2 arrow | 0-1 | 936M | DeepSeek-R1-0528 |
| math | 239,467 | 2 arrow | 0-1 | 485M | DeepSeek-R1-0528 |
| multilingual_de | 1,015,314 | 38 arrow | 0-37 | 18G | DeepSeek-R1-0528, Qwen2.5-32B-Instruct-AWQ |
| multilingual_es | 935,704 | 33 arrow | 0-32 | 16G | DeepSeek-R1-0528, Qwen2.5-14B-Instruct |
| multilingual_fr | 1,001,504 | 37 arrow | 0-36 | 17G | DeepSeek-R1-0528, Qwen2.5-14B-Instruct |
| multilingual_it | 1,016,503 | 38 arrow | 0-37 | 18G | DeepSeek-R1-0528, Qwen2.5-14B-Instruct |
| multilingual_ja | 975,202 | 37 arrow | 0-36 | 17G | DeepSeek-R1-0528, Qwen2.5-14B-Instruct |
| stem | 355,000 | 2 arrow | 0-1 | 771M | DeepSeek-R1-0528 |

**Split Descriptions**:
- **chat**: Enhanced conversational data with multiple generator models
- **code**: Programming tasks (smaller than v1, supplementary)
- **math**: Mathematical reasoning (smaller than v1, supplementary)
- **multilingual_de**: German language content (~16% of v2)
- **multilingual_es**: Spanish language content (~15% of v2)
- **multilingual_fr**: French language content (~16% of v2)
- **multilingual_it**: Italian language content (~16% of v2)
- **multilingual_ja**: Japanese language content (~15% of v2)
- **stem**: STEM content (smaller than v1, supplementary)

**v2 Key Differences from v1**:
- Adds 5 multilingual splits (DE, ES, FR, IT, JA) - ~79% of v2 data
- Smaller code/math/stem splits (supplementary to v1)
- Uses additional Qwen2.5 models for multilingual generation

---

### nvidia/Nemotron-Instruction-Following-Chat-v1 (v3-chat)

- **Format**: Arrow (HuggingFace)
- **Total Rows**: 430,978
- **Strategy**: `concatenate_messages`

**Columns**:
- `uuid`
- `messages` (text)
- `license`
- `used_in`
- `tools`
- `reasoning`
- `capability_target`

**Splits**:
| Split | Rows | Files | Shards |
|-------|------|-------|--------|
| chat_if | 426,009 | 14 arrow | 0-13 |
| structured_outputs | 4,969 | 1 arrow | 0 |

---

### nvidia/Nemotron-Agentic-v1 (v3-agentic)

- **Format**: JSONL
- **Total Rows**: 335,122
- **Strategy**: `concatenate_messages`

**Columns**:
- `uuid`
- `messages` (text)
- `license`
- `used_in`
- `tools`
- `reasoning`

**Splits**:
| Split | Rows | Files | Shards |
|-------|------|-------|--------|
| interactive_agent | ~167,561 | 1 jsonl | shard_size based |
| tool_calling | ~167,561 | 1 jsonl | shard_size based |

---

### nvidia/Nemotron-Science-v1 (v3-science)

- **Format**: Arrow (HuggingFace)
- **Total Rows**: 226,334
- **Strategy**: `concatenate_messages`

**Columns**:
- `uuid`
- `messages` (text)
- `license`
- `used_in`
- `tools`

**Splits**:
| Split | Rows | Files | Shards |
|-------|------|-------|--------|
| MCQ | 174,155 | 4 arrow | 0-3 |
| RQA | 52,179 | 2 arrow | 0-1 |

---

### nvidia/Nemotron-Math-Proofs-v1 (v3-math-proofs)

- **Format**: Arrow (HuggingFace)
- **Total Rows**: 1,376,663
- **Strategy**: `combine_columns`

**Columns**:
- `problem` (text)
- `source`
- `formal_statement` (text)
- `lean_header` (text)
- `url`
- `user_name`
- `user_url`
- `sft_line_number`
- `messages`
- `uuid`
- `used_in`
- `tools`
- `license`

**Splits**:
| Split | Rows | Files | Shards |
|-------|------|-------|--------|
| lean | 1,376,663 | 58 arrow | 0-57 |

---

### nvidia/Nemotron-Math-v2 (v3-math)

- **Format**: JSONL
- **Total Rows**: 7,085,839
- **Strategy**: `concatenate_messages`

**Columns**:
- `expected_answer`
- `problem`
- `original_expected_answer`
- `changed_answer_to_majority`
- `data_source`
- `messages` (text)
- `used_in`
- `metadata`
- `license`

**Splits**:
| Split | Rows | Files | Shards |
|-------|------|-------|--------|
| high.part_00 | ~1,417,168 | 1 jsonl | shard_size based |
| high.part_01 | ~1,417,168 | 1 jsonl | shard_size based |
| high.part_02 | ~1,417,168 | 1 jsonl | shard_size based |
| low | ~1,417,168 | 1 jsonl | shard_size based |
| medium | ~1,417,167 | 1 jsonl | shard_size based |

---

### nvidia/Nemotron-3-Nano-RL-Training-Blend (v3-rl-blend)

- **Format**: JSONL
- **Total Rows**: 93,244
- **Strategy**: `combine_columns`

**Columns**:
- `id`
- `responses_create_params` (text)
- `ground_truth`
- `category`
- `environment_name`
- `agent_ref`
- `pass_rate`
- `pass_rate_total`
- `pass_rate_passed`
- `dataset`

**Splits**:
| Split | Rows | Files | Shards |
|-------|------|-------|--------|
| train | 93,244 | 1 jsonl | shard_size based |

---

### nvidia/Nemotron-Competitive-Programming-v1 (v3-competitive)

- **Format**: JSONL
- **Total Rows**: 3,927,984
- **Strategy**: `concatenate_messages`

**Columns**:
- `uuid`
- `messages` (text)
- `license`
- `used_in`
- `tools`
- `dataset`
- `split`
- `index`
- `source`
- `difficulty`
- `question_id`

**Splits**:
| Split | Rows | Files | Shards |
|-------|------|-------|--------|
| competitive_coding_cpp.part_00 | ~654,664 | 1 jsonl | shard_size based |
| competitive_coding_cpp.part_01 | ~654,664 | 1 jsonl | shard_size based |
| competitive_coding_python.part_00 | ~654,664 | 1 jsonl | shard_size based |
| competitive_coding_python.part_01 | ~654,664 | 1 jsonl | shard_size based |
| infinibyte.part_00 | ~654,664 | 1 jsonl | shard_size based |
| infinibyte.part_01 | ~654,664 | 1 jsonl | shard_size based |

---

### nvidia/Nemotron-SWE-v1 (v3-swe)

- **Format**: Arrow (HuggingFace)
- **Total Rows**: 51,029
- **Strategy**: `concatenate_messages`

**Columns**:
- `uuid`
- `messages` (text)
- `license`
- `used_in`
- `tools`
- `dataset`
- `repo`

**Splits**:
| Split | Rows | Files | Shards |
|-------|------|-------|--------|
| r2e_gym | 51,029 | 21 arrow | 0-20 |

---

## Column Analysis & Text Extraction

### All Columns vs Key Text Columns Summary

| Dataset | All Columns | Key Text Columns (Combined) | Strategy |
|---------|-------------|----------------------------|----------|
| **pretrain-sample** | `id`, `text` | `text` | direct_text |
| **llama-sft** | `input`, `output`, `category`, `license`, `reasoning`, `generator`, `used_in_training`, `version`, `system_prompt` | `input` + `output` | input_output |
| **v1** | `uuid`, `license`, `generator`, `version`, `category`, `reasoning`, `messages`, `metadata` | `messages` | concatenate_messages |
| **v2** | `uuid`, `license`, `generator`, `version`, `category`, `reasoning`, `messages` | `messages` | concatenate_messages |
| **v3-instruction-chat** | `uuid`, `messages`, `license`, `used_in`, `tools`, `reasoning`, `capability_target` | `messages` | concatenate_messages |
| **v3-agentic** | `uuid`, `messages`, `license`, `used_in`, `tools`, `reasoning` | `messages` | concatenate_messages |
| **v3-science** | `uuid`, `messages`, `license`, `used_in`, `tools` | `messages` | concatenate_messages |
| **v3-math-proofs** | `problem`, `source`, `formal_statement`, `lean_header`, `url`, `user_name`, `user_url`, `sft_line_number`, `messages`, `uuid`, `used_in`, `tools`, `license` | `problem` + `formal_statement` + `lean_header` + `messages` | math_proofs |
| **v3-math** | `expected_answer`, `problem`, `original_expected_answer`, `changed_answer_to_majority`, `data_source`, `messages`, `tools`, `used_in`, `metadata`, `license`, `uuid`, `url`, `user_url`, `user_name` | `messages` | concatenate_messages |
| **v3-rl-blend** | `id`, `responses_create_params`, `ground_truth`, `category`, `environment_name`, `agent_ref`, `pass_rate`, `pass_rate_total`, `pass_rate_passed`, `dataset` | `responses_create_params` | combine_columns |
| **v3-competitive-programming** | `uuid`, `messages`, `license`, `used_in`, `tools`, `dataset`, `split`, `index`, `source`, `difficulty`, `question_id` | `messages` | concatenate_messages |
| **v3-swe** | `uuid`, `messages`, `license`, `used_in`, `tools`, `dataset`, `repo` | `messages` | concatenate_messages |

---

### Column Classification

| Column Type | Columns | Usage |
|-------------|---------|-------|
| **Primary Text** | `messages`, `text`, `input`, `output`, `problem`, `formal_statement` | Main content for embeddings |
| **Secondary Text** | `lean_header`, `responses_create_params`, `tools` | Additional context (dataset-specific) |
| **Metadata (skip)** | `uuid`, `license`, `generator`, `version`, `category`, `source`, `data_source`, `url`, `user_url`, `user_name`, `used_in`, `used_in_training`, `split`, `index`, `difficulty`, `question_id`, `sft_line_number`, `repo`, `dataset` | Identifiers, not for embedding |
| **Flags/Scores** | `reasoning`, `expected_answer`, `original_expected_answer`, `changed_answer_to_majority`, `capability_target`, `ground_truth`, `pass_rate`, `pass_rate_total`, `pass_rate_passed`, `environment_name`, `agent_ref` | Task metadata, not for embedding |

---

### Text Extraction Output Format

#### `concatenate_messages` (Most datasets)

```
Input: messages = [
  {role: "system", content: "..."},
  {role: "user", content: "..."},
  {role: "assistant", content: "...", reasoning_content: "..."}
]

Output:
system: <system content>
user: <user content>
assistant: <reasoning_content if exists>
<assistant content>
```

#### `math_proofs` (v3-math-proofs)

```
Input: problem, formal_statement, lean_header, messages (optional)

Output:
Problem: <problem text>

Formal Statement:
<formal_statement>

Lean Header:
<lean_header>

Messages:
user: ...
assistant: ...
```

#### `input_output` (llama-sft)

```
Input: input (list of messages or string), output (string)

Output:
user: <input content>
assistant: <output content>
```

#### `direct_text` (pretrain-sample)

```
Output: <raw text from 'text' column>
```

#### `combine_columns` (v3-rl-blend)

```
Output:
<column_name>: <column_value>
```

---

## Embedding Extraction Strategies

| Strategy | Description | Datasets |
|----------|-------------|----------|
| `concatenate_messages` | Concatenate all messages with `role: content` format | v1, v2, v3-math, v3-instruction-chat, v3-agentic, v3-science, v3-competitive-programming, v3-swe |
| `input_output` | Combine input messages + output | llama-sft |
| `math_proofs` | Combine problem, formal_statement, lean_header, AND messages | v3-math-proofs |
| `combine_columns` | Combine multiple text columns | v3-rl-blend |
| `direct_text` | Direct text extraction from `text` column | pretrain-sample |

---

## Shard Naming Convention

| Format | Strategy | Shard Naming |
|--------|----------|--------------|
| **Arrow/Parquet** (≥8 files) | 1 shard per source file | `shard_XXXXX.npy` where XXXXX = source file index |
| **Arrow/Parquet** (<8 files) | Chunk-parallel across GPUs | `shard_XXXXX.npy` interleaved (0,8,16... for GPU0) |
| **JSONL** | Accumulate until shard_size | `shard_XXXXX.npy` interleaved by GPU |

**Multi-GPU Distribution**:
- If #files ≥ #GPUs: Each GPU processes different files (file-parallel)
- If #files < #GPUs: All GPUs process all files, different chunks (chunk-parallel)

---

## Model Configuration

- **Model**: `nvidia/llama-embed-nemotron-8b`
- **Embedding Size**: 4096
- **Max Tokens**: 32,768 (32K)
- **Default Batch Size**: 1
- **Default Dtype**: bfloat16
